# intensive.yaml
#   Trainer config for Intensive Benchmarking, with the full fixed batch size of 512 (with gradient accumulation).
#   This contract exactly follows that of HF.TrainingArguments so we can pass as a simple **kwargs -- make sure this
#   continues to stay valid!
#       Reference: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments
---
training_arguments:
    # Overwrite from Top-Level Config

    # Generally sticks to order from HF.TrainingArguments() Docs, skipping over sane defaults/implicitly set args...
    do_train: true
    evaluation_strategy: steps

    # Set these based on GPU RAM available...
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16

    # For Online Evaluation, only keep around the Losses
    prediction_loss_only: true

    # Learning Rate & Optimization Parameters, assumes AdamW
    learning_rate: 5.0e-5
    weight_decay: 0.01
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1.0e-8

    # Gradient Norm
    max_grad_norm: 1.0

    # Maximum Training Steps (Overrides epochs!)
    max_steps: 1000

    # LR Scheduling Parameters
    lr_scheduler_type: linear
    warmup_steps: 100

    # Logging Parameters -- Logging Directory (Tensorboard - is this necessary?) should be Overwritten at Runtime!
    run_name: null
    logging_dir: null
    logging_first_step: true
    logging_steps: 50

    # Saving and Evaluation Steps (only at the end)
    eval_steps: 100
    save_steps: 100

    # Resume Behavior --> ignore "full determinism" on resume (saves time for debugging)
    ignore_data_skip: false

    ### Optimization -- Precision, DeepSpeed, and FairScale Parameters -- all off for `simple` config
    fp16: false

    # Dataloader Parallelism
    dataloader_num_workers: 4
