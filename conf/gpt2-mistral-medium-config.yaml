# gpt2-mistral-medium-config.yaml
#   Full Mistral GPT-2 Medium Training Config, currently working with the OpenWebText Dataset, GPT-2 Medium
#   Architecture, and full batch size (512). Runs with DeepSpeed ZeRO-2, with a per-device BSZ of 8, with partial
#   gradient checkpointing (4 / 24 blocks).
#
#   Inheritance and core paths can all be overridden from the command line or by re-writing these files.
---
# Inherit Dataset, Tokenization, Model, and Training Details
inherit:
    - datasets/openwebtext.yaml
    - models/gpt2-medium.yaml
    - trainers/gpt2-medium.yaml

# Run ID -- make sure to override!
run_id: null

# Weights & Biases
wandb: mistral-gpt2
group: gpt2-medium

# Artifacts & Caching
artifacts:
    cache_dir: /scr-ssd/mercury/mistral/artifacts
    run_dir: /scr-ssd/mercury/mistral/runs

# Save Effective Batch Size for Easy Handling ==> Main Code asserts infra + training_config results in this!
effective_bsz: 512

# Resume from Checkpoint
resume: false
resume_checkpoint: null
checkpoint_frequency:
    - [10, 100]
    - [50, 2000]
    - [100, 20000]
    - [1000, 400000]

# `torch.distributed` Default Infra Parameters -- to be overwritten by call to `torch.distributed.launch`
local_rank: -1
nnodes: -1
nproc_per_node: -1

# DeepSpeed Default Infra Parameters -- to be overwritten by call to `DeepSpeed`
num_gpus: -1
num_nodes: -1
world_size: -1

# Logging Parameters -- 10 = DEBUG, 20 = INFO, 30 = WARNING, 40 = ERROR, 50 = CRITICAL
log_level: 20

# Random Seed
seed: 21
