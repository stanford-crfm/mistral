# gpt2-small.yaml
#   Trainer config for Full GPT-2 Small, with the full fixed batch size of 512 (with gradient accumulation).
#   This contract exactly follows that of HF.TrainingArguments so we can pass as a simple **kwargs -- make sure this
#   continues to stay valid!
#       Reference: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments
---
training_arguments:
    # Overwrite from Top-Level Config
    output_dir: null

    # Generally sticks to order from HF.TrainingArguments() Docs, skipping over sane defaults/implicitly set args...
    do_train: true
    evaluation_strategy: steps

    # Set these based on GPU RAM/your available hardware
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 16

    # We set this dynamically based on DDP Computation [steps = effective_batch / (per_gpu_batch * gpus * nodes)]
    gradient_accumulation_steps: null

    # For Online Evaluation, only keep around the Losses
    prediction_loss_only: true

    # Learning Rate & Optimization Parameters, assumes AdamW
    learning_rate: 0.0006
    weight_decay: 0.1
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1.0e-8

    # Gradient Norm
    max_grad_norm: 1.0

    # Maximum Training Steps (Overrides epochs!)
    max_steps: 400000

    # LR Scheduling Parameters -- Warmup Steps should be 1% of total steps (Could use ratio)
    lr_scheduler_type: linear   # Cosine not supported if we want to use DeepSpeed Optimizers (gets overwritten!)
    warmup_steps: 4000

    # Logging Parameters -- Logging Directory (Tensorboard - is this necessary?) should be Overwritten at Runtime!
    run_name: null
    logging_dir: null
    logging_first_step: true
    logging_steps: 50

    # Saving and Evaluation Steps
    eval_steps: 1000
    save_steps: 1000

    # Resume Behavior --> ignore "full determinism" on resume (saves time for debugging)
    ignore_data_skip: false

    # Seeds -- Should be Overwritten at Runtime!
    seed: null

    ### Optimization -- Precision, DeepSpeed, and FairScale Parameters -- all off for `simple` config
    fp16: true
    sharded_ddp: null
    deepspeed: null

    # Dataloader Parallelism
    dataloader_num_workers: 0

    # Should be overwritten from the Top-Level Config or CLI!
    local_rank: null
