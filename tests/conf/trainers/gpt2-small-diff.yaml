# gpt2-small-diff.yaml
#   Trainer config for Full GPT-2 Small, with the full fixed batch size of 512 (with gradient accumulation).
#   This contract exactly follows that of HF.TrainingArguments so we can pass as a simple **kwargs -- make sure this
#   continues to stay valid!
#       Reference: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments
---

inherit:
    - gpt2-small.yaml

training_arguments:
    # Learning Rate & Optimization Parameters, assumes AdamW
    weight_decay: 0.2
    adam_beta1: 0.7
    adam_beta2: 0.3

    # Gradient Norm
    max_grad_norm: 2.0

    # Maximum Training Steps (Overrides epochs!)
    max_steps: 100000